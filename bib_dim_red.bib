
@article{ren_non-negative_2018,
	title = {Non-negative {Matrix} {Factorization}: {Robust} {Extraction} of {Extended} {Structures}},
	volume = {852},
	issn = {0004-637X},
	shorttitle = {Non-negative {Matrix} {Factorization}},
	url = {https://doi.org/10.3847%2F1538-4357%2Faaa1f2},
	doi = {10.3847/1538-4357/aaa1f2},
	abstract = {We apply the vectorized non-negative matrix factorization (NMF) method to the post-processing of the direct imaging data of exoplanetary systems such as circumstellar disks. NMF is an iterative approach, which first creates a nonorthogonal and non-negative basis of components using the given reference images and then models a target with the components. The constructed model is then rescaled with a factor to compensate for the contribution from the disks. We compare NMF with existing methods (classical reference differential imaging method, and the Karhunen–Loève image projection algorithm) using synthetic circumstellar disks and demonstrate the superiority of NMF: with no need of prior selection of references, NMF not only can detect fainter circumstellar disks but also better preserves their morphology and does not require forward modeling. As an application to a well-known disk example, we process the archival Hubble Space Telescope STIS coronagraphic observations of HD 181327 with different methods and compare them, and NMF is able to extract some circumstellar materials inside the primary ring for the first time. In an appendix, we mathematically investigate the stability of NMF components during the iteration and the linearity of NMF modeling.},
	language = {en},
	number = {2},
	urldate = {2019-10-28},
	journal = {The Astrophysical Journal},
	author = {Ren, Bin and Pueyo, Laurent and Zhu, Guangtun Ben and Debes, John and Duchêne, Gaspard},
	month = jan,
	year = {2018},
	pages = {104}
}

@article{stein-obrien_enter_2018,
	title = {Enter the {Matrix}: {Factorization} {Uncovers} {Knowledge} from {Omics}},
	volume = {34},
	issn = {0168-9525},
	shorttitle = {Enter the {Matrix}},
	url = {https://www.cell.com/trends/genetics/abstract/S0168-9525(18)30124-0},
	doi = {10.1016/j.tig.2018.07.003},
	language = {English},
	number = {10},
	urldate = {2019-10-31},
	journal = {Trends in Genetics},
	author = {Stein-O’Brien, Genevieve L. and Arora, Raman and Culhane, Aedin C. and Favorov, Alexander V. and Garmire, Lana X. and Greene, Casey S. and Goff, Loyal A. and Li, Yifeng and Ngom, Aloune and Ochs, Michael F. and Xu, Yanxun and Fertig, Elana J.},
	month = oct,
	year = {2018},
	pmid = {30143323},
	keywords = {deconvolution, dimension reduction, genomics, matrix factorization, single cell, unsupervised learning},
	pages = {790--805}
}

@incollection{xue_cell-based_2004,
	address = {Totowa, NJ},
	series = {Methods in {Molecular} {Biology}™},
	title = {Cell-{Based} {Partitioning}},
	isbn = {978-1-59259-802-1},
	url = {https://doi.org/10.1385/1-59259-802-1:279},
	abstract = {Partitioning techniques are widely used to classify compound sets or databases according to specific chemical or biological criteria. Partitioning is conceptually related to, yet algorithmically distinct from, conventional clustering methods and is particularly suitable for efficient processing of very large compound sets. Currently, some of the most popular partitioning approaches in the chemoinformatics field involve dimension reduction of initially defined chemistry spaces and creation of subsections of low-dimensional space for molecular classification. These subsections are often called cells. Original chemical reference spaces are generated through selection of various descriptors of molecular structure and properties. Principles and methodological aspects of dimension reduction of chemical spaces and compound partitioning in low-dimensional space are described herein.},
	language = {en},
	urldate = {2019-12-23},
	booktitle = {Chemoinformatics: {Concepts}, {Methods}, and {Tools} for {Drug} {Discovery}},
	publisher = {Humana Press},
	author = {Xue, Ling and Stahura, Florence L. and Bajorath, Jürgen},
	editor = {Bajorath, Jürgen},
	year = {2004},
	doi = {10.1385/1-59259-802-1:279},
	keywords = {dimension reduction, Biological activity, chemical features, chemical space, cluster analysis, compound databases, molecular descriptors, molecule classification, partitioning algorithms, partitioning in low-dimensional spaces, principal component analysis, visualization},
	pages = {279--289}
}

@article{comon_independent_1994,
	series = {Higher {Order} {Statistics}},
	title = {Independent component analysis, {A} new concept?},
	volume = {36},
	issn = {0165-1684},
	url = {http://www.sciencedirect.com/science/article/pii/0165168494900299},
	doi = {10.1016/0165-1684(94)90029-9},
	abstract = {The independent component analysis (ICA) of a random vector consists of searching for a linear transformation that minimizes the statistical dependence between its components. In order to define suitable search criteria, the expansion of mutual information is utilized as a function of cumulants of increasing orders. An efficient algorithm is proposed, which allows the computation of the ICA of a data matrix within a polynomial time. The concept of ICA may actually be seen as an extension of the principal component analysis (PCA), which can only impose independence up to the second order and, consequently, defines directions that are orthogonal. Potential applications of ICA include data analysis and compression, Bayesian detection, localization of sources, and blind identification and deconvolution.
Zusammenfassung
Die Analyse unabhängiger Komponenten (ICA) eines Vektors beruht auf der Suche nach einer linearen Transformation, die die statistische Abhängigkeit zwischen den Komponenten minimiert. Zur Definition geeigneter Such-Kriterien wird die Entwicklung gemeinsamer Information als Funktion von Kumulanten steigender Ordnung genutzt. Es wird ein effizienter Algorithmus vorgeschlagen, der die Berechnung der ICA für Datenmatrizen innerhalb einer polynomischen Zeit erlaubt. Das Konzept der ICA kann eigentlich als Erweiterung der ‘Principal Component Analysis‘ (PCA) betrachtet werden, die nur die Unabhängigkeit bis zur zweiten Ordnung erzwingen kann und deshalb Richtungen definiert, die orthogonal sind. Potentielle Anwendungen der ICA beinhalten Daten-Analyse und Kompression, Bayes-Detektion, Quellenlokalisierung und blinde Identifikation und Entfaltung.
Résumé
L'Analyse en Composantes Indépendentes (ICA) d'un vecteur aléatoire consiste en la recherche d'une transformation linéaire qui minimise la dépendance statistique entre ses composantes. Afin de définir des critères d'optimisation appropriés, on utilise un développment en série de l'information mutuelle en fonction de cumulants d'ordre croissant. On propose ensuite un algorithme pratique permettant le calcul de l'ICA d'une matrice de données en un temps polynomial. Le concept d'ICA peut être vu en réalité comme une extention de l'Analyse en Composantes Principales (PCA) qui, elle, ne peut imposer l'indépendence qu'au second ordre et définit par conséquent des directions orthogonales. Les applications potentielles de l'ICA incluent l'analyse et la compression de données, la détection bayesienne, la localisation de sources, et l'identification et la déconvolution aveugles.},
	language = {en},
	number = {3},
	urldate = {2019-12-25},
	journal = {Signal Processing},
	author = {Comon, Pierre},
	month = apr,
	year = {1994},
	pages = {287--314}
}

@article{liland_multivariate_2011,
	title = {Multivariate methods in metabolomics – from pre-processing to dimension reduction and statistical analysis},
	volume = {30},
	issn = {0165-9936},
	url = {http://www.sciencedirect.com/science/article/pii/S0165993611000914},
	doi = {10.1016/j.trac.2011.02.007},
	abstract = {This article presents some of the multivariate methods used in metabolomics, and addresses many of the data types and associated analyses of current instrumentation and applications seen from the point of view of data analysis. I cover most of the statistical pipeline – from pre-processing to the final results of statistical analysis (i.e. pre-processing of the data, regression, classification, clustering, validation and related subjects). Most emphasis is on descriptions of the methods, their advantages and weaknesses, and their usefulness in metabolomics. Of course, the selection of methods presented is not an exhaustive, but should shed some light on some of the more popular and relevant.},
	language = {en},
	number = {6},
	urldate = {2019-12-25},
	journal = {TrAC Trends in Analytical Chemistry},
	author = {Liland, Kristian Hovde},
	month = jun,
	year = {2011},
	keywords = {Metabolomics, Data analysis, Data classification, Data clustering, Data validation, Dimension reduction, Multivariate method, Pre-processing data, Regression, Statistical analysis},
	pages = {827--841}
}

@article{boccard_harnessing_2014,
	title = {Harnessing the complexity of metabolomic data with chemometrics},
	volume = {28},
	issn = {0886-9383},
	url = {https://onlinelibrary.wiley.com/doi/full/10.1002/cem.2567},
	doi = {10.1002/cem.2567},
	abstract = {Because of the ever-increasing number of signals that can be measured within a single run by modern platforms in analytical chemistry, life sciences datasets become not only gradually larger but also more intricate in their structures. Challenges related to making use of this wealth of data include extracting relevant elements within massive amounts of signals possibly spread across different tables, reducing dimensionality, summarising dynamic information in a comprehensible way and displaying it for interpretation purposes. Metabolomics constitutes a representative example of fast-moving research fields taking advantage of recent technological advances to provide extensive sample monitoring. Because of the wide chemical diversity of metabolites, several analytical setups are required to provide a broad coverage of complex samples. The integration and visualisation of multiple highly multivariate datasets constitute key issues for effective analysis leading to valuable biological or chemical knowledge. Additionally, high-order data structures arise from experimental setups involving time-resolved measurements. These data are intrinsically multiway, and classical statistical tools cannot be applied without altering their organisation with the risk of information loss. Dedicated modelling algorithms, able to cope with the inherent properties of these metabolomic datasets, are therefore mandatory for harnessing their complexity and provide relevant information. In that perspective, chemometrics has a central role to play. Copyright ? 2013 John Wiley \& Sons, Ltd.},
	number = {1},
	urldate = {2019-12-25},
	journal = {Journal of Chemometrics},
	author = {Boccard, Julien and Rudaz, Serge},
	month = jan,
	year = {2014},
	keywords = {chemometrics, metabolomics, data integration, data structure, dimensionality reduction},
	pages = {1--9}
}

@incollection{cattell_extracting_1978,
	address = {Boston, MA},
	title = {Extracting {Factors}: {The} {Algebraic} {Picture}},
	isbn = {978-1-4684-2262-7},
	shorttitle = {Extracting {Factors}},
	url = {https://doi.org/10.1007/978-1-4684-2262-7_2},
	abstract = {Typically, the factor analyst takes a careful choice of variables in the area of behavior in which he is out to test a theory or to explore structure, e.g., primary ability variables; marker variables for a supposed extravert temperament; measures which psychologists have thought to be indicators of strength of motivation; or representatives of an hypothesized learning gain pattern. As he looks at the square matrix of all correlations among, say, thirty such variables, he wants to know how many distinct independent influences—presumably decidedly fewer than thirty—can be considered responsible for the observed covariations in this domain of behavior.},
	language = {en},
	urldate = {2019-12-25},
	booktitle = {The {Scientific} {Use} of {Factor} {Analysis} in {Behavioral} and {Life} {Sciences}},
	publisher = {Springer US},
	author = {Cattell, Raymond B.},
	editor = {Cattell, Raymond B.},
	year = {1978},
	doi = {10.1007/978-1-4684-2262-7_2},
	keywords = {Common Element, Common Factor, Factor Matrix, Factor Score, Unique Variance},
	pages = {15--39}
}

@book{child_essentials_2006,
	address = {New York},
	title = {The essentials of factor analysis},
	isbn = {978-0-8264-8000-2},
	publisher = {Continuum},
	author = {Child, Dennis},
	year = {2006},
	keywords = {Factor analysis, Factoranalyse}
}

@article{gaujoux_flexible_2010,
	title = {A flexible {R} package for nonnegative matrix factorization},
	volume = {11},
	issn = {1471-2105},
	url = {https://doi.org/10.1186/1471-2105-11-367},
	doi = {10.1186/1471-2105-11-367},
	abstract = {Nonnegative Matrix Factorization (NMF) is an unsupervised learning technique that has been applied successfully in several fields, including signal processing, face recognition and text mining. Recent applications of NMF in bioinformatics have demonstrated its ability to extract meaningful information from high-dimensional data such as gene expression microarrays. Developments in NMF theory and applications have resulted in a variety of algorithms and methods. However, most NMF implementations have been on commercial platforms, while those that are freely available typically require programming skills. This limits their use by the wider research community.},
	number = {1},
	urldate = {2019-12-25},
	journal = {BMC Bioinformatics},
	author = {Gaujoux, Renaud and Seoighe, Cathal},
	month = jul,
	year = {2010},
	pages = {367}
}

@article{juarez_novel_2017,
	title = {A novel approach to analyzing lung cancer mortality disparities: {Using} the exposome and a graph-theoretical toolchain},
	volume = {2},
	issn = {2468-5690},
	shorttitle = {A novel approach to analyzing lung cancer mortality disparities},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5687093/},
	abstract = {Objectives
The aim is to identify exposures associated with lung cancer mortality and mortality disparities by race and gender using an exposome database coupled to a graph theoretical toolchain.

Methods
Graph theoretical algorithms were employed to extract paracliques from correlation graphs using associations between 2162 environmental exposures and lung cancer mortality rates in 2067 counties, with clique doubling applied to compute an absolute threshold of significance. Factor analysis and multiple linear regressions then were used to analyze differences in exposures associated with lung cancer mortality and mortality disparities by race and gender.

Results
While cigarette consumption was highly correlated with rates of lung cancer mortality for both white men and women, previously unidentified novel exposures were more closely associated with lung cancer mortality and mortality disparities for blacks, particularly black women.

Conclusions
Exposures beyond smoking moderate lung cancer mortality and mortality disparities by race and gender.

Policy Implications
An exposome approach and database coupled with scalable combinatorial analytics provides a powerful new approach for analyzing relationships between multiple environmental exposures, pathways and health outcomes. An assessment of multiple exposures is needed to appropriately translate research findings into environmental public health practice and policy.},
	number = {2},
	urldate = {2020-01-02},
	journal = {Environmental disease},
	author = {Juarez, Paul D and Hood, Darryl B and Rogers, Gary L and Baktash, Suzanne H and Saxton, Arnold M and Matthews-Juarez, Patricia and Im, Wansoo and Cifuentes, Myriam Patricia and Phillips, Charles A and Lichtveld, Maureen Y and Langston, Michael A},
	year = {2017},
	pmid = {29152601},
	pmcid = {PMC5687093},
	pages = {33--44}
}

@article{lee_identification_2017,
	title = {Identification of chemical mixtures to which {Canadian} pregnant women are exposed: {The} {MIREC} {Study}},
	volume = {99},
	issn = {0160-4120},
	shorttitle = {Identification of chemical mixtures to which {Canadian} pregnant women are exposed},
	url = {http://www.sciencedirect.com/science/article/pii/S016041201631039X},
	doi = {10.1016/j.envint.2016.12.015},
	abstract = {Depending on the chemical and the outcome, prenatal exposures to environmental chemicals can lead to adverse effects on the pregnancy and child development, especially if exposure occurs during early gestation. Instead of focusing on prenatal exposure to individual chemicals, more studies have taken into account that humans are exposed to multiple environmental chemicals on a daily basis. The objectives of this analysis were to identify the pattern of chemical mixtures to which women are exposed and to characterize women with elevated exposures to various mixtures. Statistical techniques were applied to 28 chemicals measured simultaneously in the first trimester and socio-demographic factors of 1744 participants from the Maternal-Infant Research on Environment Chemicals (MIREC) Study. Cluster analysis was implemented to categorize participants based on their socio-demographic characteristics, while principal component analysis (PCA) was used to extract the chemicals with similar patterns and to reduce the dimension of the dataset. Next, hypothesis testing determined if the mean converted concentrations of chemical substances differed significantly among women with different socio-demographic backgrounds as well as among clusters. Cluster analysis identified six main socio-demographic clusters. Eleven components, which explained approximately 70\% of the variance in the data, were retained in the PCA. Persistent organic pollutants (PCB118, PCB138, PCB153, PCB180, OXYCHLOR and TRANSNONA) and phthalates (MEOHP, MEHHP and MEHP) dominated the first and second components, respectively, and the first two components explained 25.8\% of the source variation. Prenatal exposure to persistent organic pollutants (first component) were positively associated with women who have lower education or higher income, were born in Canada, have BMI ≥25, or were expecting their first child in our study population. MEOHP, MEHHP and MEHP, dominating the second component, were detected in at least 98\% of 1744 participants in our cohort study; however, no particular group of pregnant women was identified to be highly exposed to phthalates. While widely recognized as important to studying potential health effects, identifying the mixture of chemicals to which various segments of the population are exposed has been problematic. We present an approach using factor analysis through principal component method and cluster analysis as an attempt to determine the pregnancy exposome. Future studies should focus on how to include these matrices in examining the health effects of prenatal exposure to chemical mixtures in pregnant women and their children.},
	language = {en},
	urldate = {2020-01-05},
	journal = {Environment International},
	author = {Lee, Wan-Chen and Fisher, Mandy and Davis, Karelyn and Arbuckle, Tye E. and Sinha, Sanjoy K.},
	month = feb,
	year = {2017},
	keywords = {Pregnancy, Chemicals, Mixtures},
	pages = {321--330}
}

@article{bechaux_identification_2013,
	title = {Identification of pesticide mixtures and connection between combined exposure and diet},
	volume = {59},
	issn = {0278-6915},
	url = {http://www.sciencedirect.com/science/article/pii/S0278691513003724},
	doi = {10.1016/j.fct.2013.06.006},
	abstract = {The identification of the major associations of pesticides to which the population is exposed is the first step for the risk assessment of mixtures. Moreover, the interpretation of the mixtures through the individuals’ diet and the characterization of potentially high-risk populations constitute a useful tool for risk management. This paper proposes a method based on Non-Negative Matrix Factorization which allows the identification of the major mixtures to which the French population is exposed and the connection between this exposure and the diet. Exposure data of the French population are provided by the Second French Total Diet Study. The NMF is implemented on consumption data to extract consumption systems which are combined with the residue levels to link dietary behavior with exposure to mixtures of pesticides. A clustering of the individuals is achieved in order to highlight clusters of individuals with similar exposure to pesticides/consumption habits. The model provides 6 main consumption systems, 6 associated mixtures of pesticides and the description of the population which is most exposed to each mixture. Two different ways to estimate the matrix providing the mixtures of pesticides to which the population is exposed are suggested. Their advantages in different contexts of risk assessment are discussed.},
	language = {en},
	urldate = {2020-01-05},
	journal = {Food and Chemical Toxicology},
	author = {Béchaux, Camille and Zetlaoui, Mélanie and Tressou, Jessica and Leblanc, Jean-Charles and Héraud, Fanny and Crépet, Amélie},
	month = sep,
	year = {2013},
	keywords = {Combined exposure, Dietary patterns, Food risk assessment, Non-Negative Matrix Factorization, Pesticides mixtures},
	pages = {191--198}
}

@article{lee_learning_1999,
	title = {Learning the parts of objects by non-negative matrix factorization},
	volume = {401},
	copyright = {1999 Macmillan Magazines Ltd.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/44565},
	doi = {10.1038/44565},
	abstract = {Is perception of the whole based on perception of its parts? There is psychological1 and physiological2,3 evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations4,5. But little is known about how brains or computers might learn the parts of objects. Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign.},
	language = {en},
	number = {6755},
	urldate = {2020-01-05},
	journal = {Nature},
	author = {Lee, Daniel D. and Seung, H. Sebastian},
	month = oct,
	year = {1999},
	pages = {788--791}
}

@incollection{saul_multiplicative_2002,
	title = {Multiplicative {Updates} for {Classification} by {Mixture} {Models}},
	url = {http://papers.nips.cc/paper/2085-multiplicative-updates-for-classification-by-mixture-models.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 14},
	publisher = {MIT Press},
	author = {Saul, Lawrence K. and Lee, Daniel D.},
	editor = {Dietterich, T. G. and Becker, S. and Ghahramani, Z.},
	year = {2002},
	pages = {897--904}
}

@article{robinson_urban_2018,
	title = {The urban exposome during pregnancy and its socioeconomic determinants},
	volume = {126},
	number = {7},
	journal = {Environmental health perspectives},
	author = {Robinson, Oliver and Tamayo, Ibon and De Castro, Montserrat and Valentin, Antonia and Giorgis-Allemand, Lise and Hjertager Krog, Norun and Marit Aasvang, Gunn and Ambros, Albert and Ballester, Ferran and Bird, Pippa},
	year = {2018},
	pages = {077005}
}

@article{chesler_systems_2006,
	title = {Systems biology and regulatory genomics},
	author = {Chesler, E. J. and Langston, M. A. and Eskin, E. and Ideker, T. and Raphael, B. and Workman, C.},
	year = {2006}
}

@article{hagan_lower_2016,
	title = {Lower bounds on paraclique density},
	volume = {204},
	journal = {Discrete Applied Mathematics},
	author = {Hagan, Ronald D. and Langston, Michael A. and Wang, Kai},
	year = {2016},
	pages = {208--212}
}

@article{borate_comparison_2009,
	title = {Comparison of threshold selection methods for microarray gene co-expression matrices},
	volume = {2},
	number = {1},
	journal = {BMC research notes},
	author = {Borate, Bhavesh R. and Chesler, Elissa J. and Langston, Michael A. and Saxton, Arnold M. and Voy, Brynn H.},
	year = {2009},
	pages = {240}
}

@book{jolliffe_principal_2002,
	address = {New York},
	edition = {2},
	series = {Springer {Series} in {Statistics}},
	title = {Principal {Component} {Analysis}},
	isbn = {978-0-387-95442-4},
	url = {https://www.springer.com/gp/book/9780387954424},
	abstract = {Principal component analysis is central to the study of multivariate data. Although one of the earliest multivariate techniques, it continues to be the subject of much research, ranging from new model-based approaches to algorithmic ideas from neural networks. It is extremely versatile, with applications in many disciplines. The first edition of this book was the first comprehensive text written solely on principal component analysis. The second edition updates and substantially expands the original version, and is once again the definitive text on the subject. It includes core material, current research and a wide range of applications. Its length is nearly double that of the first edition. Researchers in statistics, or in other fields that use principal component analysis, will find that the book gives an authoritative yet accessible account of the subject. It is also a valuable resource for graduate courses in multivariate analysis. The book requires some knowledge of matrix algebra. Ian Jolliffe is Professor of Statistics at the University of Aberdeen. He is author or co-author of over 60 research papers and three other books. His research interests are broad, but aspects of principal component analysis have fascinated him and kept him busy for over 30 years.},
	language = {en},
	urldate = {2020-01-05},
	publisher = {Springer-Verlag},
	author = {Jolliffe, I. T.},
	year = {2002},
	doi = {10.1007/b98835}
}

@article{sobus_using_2019,
	title = {Using prepared mixtures of {ToxCast} chemicals to evaluate non-targeted analysis ({NTA}) method performance},
	volume = {411},
	number = {4},
	journal = {Analytical and bioanalytical chemistry},
	author = {Sobus, Jon R. and Grossman, Jarod N. and Chao, Alex and Singh, Randolph and Williams, Antony J. and Grulke, Christopher M. and Richard, Ann M. and Newton, Seth R. and McEachran, Andrew D. and Ulrich, Elin M.},
	year = {2019},
	pages = {835--851}
}

@article{yu_aplcmsadaptive_2009,
	title = {{apLCMS}—adaptive processing of high-resolution {LC}/{MS} data},
	volume = {25},
	issn = {1367-4803},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2712336/},
	doi = {10.1093/bioinformatics/btp291},
	abstract = {Motivation: Liquid chromatography-mass spectrometry (LC/MS) profiling is a promising approach for the quantification of metabolites from complex biological samples. Significant challenges exist in the analysis of LC/MS data, including noise reduction, feature identification/ quantification, feature alignment and computation efficiency., Result: Here we present a set of algorithms for the processing of high-resolution LC/MS data. The major technical improvements include the adaptive tolerance level searching rather than hard cutoff or binning, the use of non-parametric methods to fine-tune intensity grouping, the use of run filter to better preserve weak signals and the model-based estimation of peak intensities for absolute quantification. The algorithms are implemented in an R package apLCMS, which can efficiently process large LC/ MS datasets., Availability: The R package apLCMS is available at www.sph.emory.edu/apLCMS., Contact: tyu8@sph.emory.edu, Supplementary information: Supplementary data are available at Bioinformatics online.},
	number = {15},
	urldate = {2020-01-06},
	journal = {Bioinformatics},
	author = {Yu, Tianwei and Park, Youngja and Johnson, Jennifer M. and Jones, Dean P.},
	month = aug,
	year = {2009},
	pmid = {19414529},
	pmcid = {PMC2712336},
	pages = {1930--1936}
}

@article{barshan_supervised_2011,
	title = {Supervised principal component analysis: {Visualization}, classification and regression on subspaces and submanifolds},
	volume = {44},
	issn = {0031-3203},
	shorttitle = {Supervised principal component analysis},
	url = {http://www.sciencedirect.com/science/article/pii/S0031320310005819},
	doi = {10.1016/j.patcog.2010.12.015},
	abstract = {We propose “supervised principal component analysis (supervised PCA)”, a generalization of PCA that is uniquely effective for regression and classification problems with high-dimensional input data. It works by estimating a sequence of principal components that have maximal dependence on the response variable. The proposed supervised PCA is solvable in closed-form, and has a dual formulation that significantly reduces the computational complexity of problems in which the number of predictors greatly exceeds the number of observations (such as DNA microarray experiments). Furthermore, we show how the algorithm can be kernelized, which makes it applicable to non-linear dimensionality reduction tasks. Experimental results on various visualization, classification and regression problems show significant improvement over other supervised approaches both in accuracy and computational efficiency.},
	language = {en},
	number = {7},
	urldate = {2020-01-09},
	journal = {Pattern Recognition},
	author = {Barshan, Elnaz and Ghodsi, Ali and Azimifar, Zohreh and Zolghadri Jahromi, Mansoor},
	month = jul,
	year = {2011},
	keywords = {Dimensionality reduction, Regression, Classification, Kernel methods, Principal component analysis (PCA), Supervised learning, Visualization},
	pages = {1357--1371}
}

@article{bellman_dynamic_1966,
	title = {Dynamic programming},
	volume = {153},
	number = {3731},
	journal = {Science},
	author = {Bellman, Richard},
	year = {1966},
	pages = {34--37}
}

@book{bellman_dynamic_1972,
	edition = {sixth},
	title = {Dynamic {Programming}},
	isbn = {978-0-691-14668-3},
	url = {https://press.princeton.edu/books/paperback/9780691146683/dynamic-programming},
	language = {en},
	urldate = {2020-01-10},
	publisher = {Princeton University Press},
	author = {Bellman, Richard},
	year = {1972}
}

@article{mulaik_brief_1987,
	title = {A {Brief} {History} of the {Philosophical} {Foundations} of {Exploratory} {Factor} {Analysis}},
	volume = {22},
	issn = {0027-3171},
	doi = {10.1207/s15327906mbr2203_3},
	abstract = {Exploratory factor analysis derives its key ideas from many sources. From the Greek rationalists and atomists comes the idea that appearance is to be explained by something not observed. From Aristotle comes the idea of induction and seeking common features of things as explanations of them. From Francis Bacon comes the idea of an automatic algorithm for inductively discovering common causes. From Descartes come the ideas of analysis and synthesis that underlie the emphasis on analysis of variables into orthogonal or linearly independent factors and focus on reproducing (synthesizing) the correlation matrix from the factors. From empiricist statisticians like Pearson and Yule comes the idea of exploratory, descriptive statistics. Also from the empiricist heritage comes the false expectation some have that factor analysis yields unique and unambiguous knowledge without prior assumptions -- the inductivist fallacy. This expectation founders on the indeterminacy of factors, even after their loadings are defined by rotation. Indeterminacy is unavoidable in the interpretation of common factors because the process of interpretation is inductive and inductive inferences are not uniquely determined by the data on which they are based. But from Kant we learn not to discard inductive inferences but to treat them as hypotheses that must be tested against additional data to establish their objectivity. And so the conclusions of exploratory factor analyses are never complete without a subsequent confirmatory analysis with additional variables and new data.},
	language = {eng},
	number = {3},
	journal = {Multivariate Behavioral Research},
	author = {Mulaik, S. A.},
	month = jul,
	year = {1987},
	pmid = {26776378},
	pages = {267--305}
}

@book{thompson_exploratory_2004,
	title = {Exploratory and confirmatory factor analysis: {Understanding} concepts and applications.},
	isbn = {1-59147-093-5},
	publisher = {American Psychological Association},
	author = {Thompson, Bruce},
	year = {2004}
}

@article{li_persistent_2019,
	title = {Persistent {Organic} {Pollutants} in {Human} {Breast} {Milk} and {Associations} with {Maternal} {Thyroid} {Hormone} {Homeostasis}},
	issn = {0013-936X},
	url = {https://doi.org/10.1021/acs.est.9b06054},
	doi = {10.1021/acs.est.9b06054},
	abstract = {Epidemiological studies have indicated the thyroid-disrupting effects of persistent organic pollutants (POPs). However, the association of low-exposure POPs with thyroid hormones (THs) remains unclear. Here, we aim to assess the association of low exposure of POPs, including polybrominated diphenyl ethers (PBDEs), polychlorinated biphenyls (PCBs), polychlorinated dibenzo-p-dioxins and furans (PCDD/Fs), and polybrominated dibenzo-p-dioxins and furans, with THs [total L-thyroxine (TT4), total 3,3′,5-triiodo-L-thyronine (TT3), and total 3,3′,5′-triiodo-L-thyronine (TrT3)] measured in human breast milk. Ninety-nine breast milk samples were collected from the LUPE cohort (2015–2016, Bavaria, Germany). Fourteen PBDEs, 17 PCBs, and 5 PCDD/Fs had quantification rates of {\textgreater}80\%. Nonmonotonic associations were observed. In adjusted single-pollutant models, (1) TT4 was inversely associated with BDE-99, -154, and -196; (2) TT3 was inversely associated with BDE-47, -99, -100, -197, -203, -207, and OCDD; and (3) TrT3 was inversely associated with BDE-47, -99, -183, and -203. Multipollutant analysis using principal component analysis and hierarchical clustering revealed inverse associations of PBDEs (BDE-28, -47, -99, -100, -154, -183, and -197) with TT4 and TrT3. These results indicate that POPs at low levels might be related to reduced THs. This study shows that human breast milk might be an appropriate specimen to evaluate the thyroid disruption of POPs.},
	urldate = {2020-01-10},
	journal = {Environmental Science \& Technology},
	author = {Li, Zhong-Min and Albrecht, Michael and Fromme, Hermann and Schramm, Karl-Werner and De Angelis, Meri},
	month = dec,
	year = {2019}
}

@article{li_persistent_2020,
	title = {Persistent {Organic} {Pollutants} in {Human} {Breast} {Milk} and {Associations} with {Maternal} {Thyroid} {Hormone} {Homeostasis}},
	issn = {1520-5851},
	doi = {10.1021/acs.est.9b06054},
	abstract = {Epidemiological studies have indicated the thyroid-disrupting effects of persistent organic pollutants (POPs). However, the association of low-exposure POPs with thyroid hormones (THs) remains unclear. Here, we aim to assess the association of low exposure of POPs, including polybrominated diphenyl ethers (PBDEs), polychlorinated biphenyls (PCBs), polychlorinated dibenzo-p-dioxins and furans (PCDD/Fs), and polybrominated dibenzo-p-dioxins and furans, with THs [total L-thyroxine (TT4), total 3,3',5-triiodo-L-thyronine (TT3), and total 3,3',5'-triiodo-L-thyronine (TrT3)] measured in human breast milk. Ninety-nine breast milk samples were collected from the LUPE cohort (2015-2016, Bavaria, Germany). Fourteen PBDEs, 17 PCBs, and 5 PCDD/Fs had quantification rates of {\textgreater}80\%. Nonmonotonic associations were observed. In adjusted single-pollutant models, (1) TT4 was inversely associated with BDE-99, -154, and -196; (2) TT3 was inversely associated with BDE-47, -99, -100, -197, -203, -207, and OCDD; and (3) TrT3 was inversely associated with BDE-47, -99, -183, and -203. Multipollutant analysis using principal component analysis and hierarchical clustering revealed inverse associations of PBDEs (BDE-28, -47, -99, -100, -154, -183, and -197) with TT4 and TrT3. These results indicate that POPs at low levels might be related to reduced THs. This study shows that human breast milk might be an appropriate specimen to evaluate the thyroid disruption of POPs.},
	language = {eng},
	journal = {Environmental Science \& Technology},
	author = {Li, Zhong-Min and Albrecht, Michael and Fromme, Hermann and Schramm, Karl-Werner and De Angelis, Meri},
	month = jan,
	year = {2020},
	pmid = {31867966}
}

@article{tamayo-uria_early-life_2019,
	title = {The early-life exposome: {Description} and patterns in six {European} countries},
	volume = {123},
	issn = {0160-4120},
	shorttitle = {The early-life exposome},
	url = {http://www.sciencedirect.com/science/article/pii/S0160412018316295},
	doi = {10.1016/j.envint.2018.11.067},
	abstract = {Characterization of the “exposome”, the set of all environmental factors that one is exposed to from conception onwards, has been advocated to better understand the role of environmental factors on chronic diseases. Here, we aimed to describe the early-life exposome. Specifically, we focused on the correlations between multiple environmental exposures, their patterns and their variability across European regions and across time (pregnancy and childhood periods). We relied on the Human Early-Life Exposome (HELIX) project, in which 87 environmental exposures during pregnancy and 122 during the childhood period (grouped in 19 exposure groups) were assessed in 1301 pregnant mothers and their children at 6–11 years in 6 European birth cohorts. Some correlations between exposures in the same exposure group reached high values above 0.8. The median correlation within exposure groups was {\textgreater}0.3 for many exposure groups, reaching 0.69 for water disinfection by products in pregnancy and 0.67 for the meteorological group in childhood. Median correlations between different exposure groups rarely reached 0.3. Some correlations were driven by cohort-level associations (e.g. air pollution and chemicals). Ten principal components explained 45\% and 39\% of the total variance in the pregnancy and childhood exposome, respectively, while 65 and 90 components were required to explain 95\% of the exposome variability. Correlations between maternal (pregnancy) and childhood exposures were high ({\textgreater}0.6) for most exposures modeled at the residential address (e.g. air pollution), but were much lower and even close to zero for some chemical exposures. In conclusion, the early life exposome was high dimensional, meaning that it cannot easily be measured by or reduced to fewer components. Correlations between exposures from different exposure groups were much lower than within exposure groups, which have important implications for co-exposure confounding in multiple exposure studies. Also, we observed the early life exposome to be variable over time and to vary by cohort, so measurements at one time point or one place will not capture its complexities.},
	language = {en},
	urldate = {2020-01-14},
	journal = {Environment International},
	author = {Tamayo-Uria, Ibon and Maitre, Léa and Thomsen, Cathrine and Nieuwenhuijsen, Mark J. and Chatzi, Leda and Siroux, Valérie and Aasvang, Gunn Marit and Agier, Lydiane and Andrusaityte, Sandra and Casas, Maribel and de Castro, Montserrat and Dedele, Audrius and Haug, Line S. and Heude, Barbara and Grazuleviciene, Regina and Gutzkow, Kristine B. and Krog, Norun H. and Mason, Dan and McEachan, Rosemary R. C. and Meltzer, Helle M. and Petraviciene, Inga and Robinson, Oliver and Roumeliotaki, Theano and Sakhi, Amrit K. and Urquiza, Jose and Vafeiadi, Marina and Waiblinger, Dagmar and Warembourg, Charline and Wright, John and Slama, Rémy and Vrijheid, Martine and Basagaña, Xavier},
	month = feb,
	year = {2019},
	keywords = {Pregnancy, Exposome, Environmental exposures, Children, Early life},
	pages = {189--200}
}

@article{smith_xcms_2006,
	title = {{XCMS}: processing mass spectrometry data for metabolite profiling using nonlinear peak alignment, matching, and identification},
	volume = {78},
	issn = {0003-2700},
	shorttitle = {{XCMS}},
	doi = {10.1021/ac051437y},
	abstract = {Metabolite profiling in biomarker discovery, enzyme substrate assignment, drug activity/specificity determination, and basic metabolic research requires new data preprocessing approaches to correlate specific metabolites to their biological origin. Here we introduce an LC/MS-based data analysis approach, XCMS, which incorporates novel nonlinear retention time alignment, matched filtration, peak detection, and peak matching. Without using internal standards, the method dynamically identifies hundreds of endogenous metabolites for use as standards, calculating a nonlinear retention time correction profile for each sample. Following retention time correction, the relative metabolite ion intensities are directly compared to identify changes in specific endogenous metabolites, such as potential biomarkers. The software is demonstrated using data sets from a previously reported enzyme knockout study and a large-scale study of plasma samples. XCMS is freely available under an open-source license at http://metlin.scripps.edu/download/.},
	language = {eng},
	number = {3},
	journal = {Analytical Chemistry},
	author = {Smith, Colin A. and Want, Elizabeth J. and O'Maille, Grace and Abagyan, Ruben and Siuzdak, Gary},
	month = feb,
	year = {2006},
	pmid = {16448051},
	keywords = {Animals, Humans, Mice, Sensitivity and Specificity, Mass Spectrometry, Time Factors, Mice, Knockout, Chromatography, Liquid, Algorithms, Amidohydrolases, Nonlinear Dynamics},
	pages = {779--787}
}

@article{uppal_xmsanalyzer_2013,
	title = {{xMSanalyzer}: automated pipeline for improved feature detection and downstream analysis of large-scale, non-targeted metabolomics data},
	volume = {14},
	issn = {1471-2105},
	shorttitle = {{xMSanalyzer}},
	url = {https://doi.org/10.1186/1471-2105-14-15},
	doi = {10.1186/1471-2105-14-15},
	abstract = {Detection of low abundance metabolites is important for de novo mapping of metabolic pathways related to diet, microbiome or environmental exposures. Multiple algorithms are available to extract m/z features from liquid chromatography-mass spectral data in a conservative manner, which tends to preclude detection of low abundance chemicals and chemicals found in small subsets of samples. The present study provides software to enhance such algorithms for feature detection, quality assessment, and annotation.},
	number = {1},
	urldate = {2020-01-14},
	journal = {BMC Bioinformatics},
	author = {Uppal, Karan and Soltow, Quinlyn A. and Strobel, Frederick H. and Pittard, W. Stephen and Gernert, Kim M. and Yu, Tianwei and Jones, Dean P.},
	month = jan,
	year = {2013},
	pages = {15}
}

@article{alonso_analytical_2015,
	title = {Analytical {Methods} in {Untargeted} {Metabolomics}: {State} of the {Art} in 2015},
	volume = {3},
	issn = {2296-4185},
	shorttitle = {Analytical {Methods} in {Untargeted} {Metabolomics}},
	url = {https://www.frontiersin.org/articles/10.3389/fbioe.2015.00023/full},
	doi = {10.3389/fbioe.2015.00023},
	abstract = {Metabolomics comprises the methods and techniques that are used to measure the small molecule composition of biofluids and tissues, and is actually one of the most rapidly evolving research fields. The determination of the metabolomic profile –the metabolome- has multiple applications in many biological sciences, including the developing of new diagnostic tools in medicine. Recent technological advances in nuclear magnetic resonance (NMR) and mass spectrometry (MS) are significantly improving our capacity to obtain more data from each biological sample. Consequently, there is a need for fast and accurate statistical and bioinformatic tools that can deal with the complexity and volume of the data generated in metabolomic studies. In this review we provide an update of the most commonly used analytical methods in metabolomics, starting from raw data processing and ending with pathway analysis and biomarker identification. Finally, the integration of metabolomic profiles with molecular data from other high throughput biotechnologies is also reviewed.},
	language = {English},
	urldate = {2020-01-14},
	journal = {Frontiers in Bioengineering and Biotechnology},
	author = {Alonso, Arnald and Marsal, Sara and Julià, Antonio},
	year = {2015},
	keywords = {Metabolomics, Mass Spectrometry, data analysis, nuclear magnetic resonance (NMR), pathway analysis, spectral processing, untargeted metabolomics}
}

@article{niedzwiecki_exposome_2019,
	title = {The {Exposome}: {Molecules} to {Populations}},
	volume = {59},
	issn = {1545-4304},
	shorttitle = {The {Exposome}},
	doi = {10.1146/annurev-pharmtox-010818-021315},
	abstract = {Derived from the term exposure, the exposome is an omic-scale characterization of the nongenetic drivers of health and disease. With the genome, it defines the phenome of an individual. The measurement of complex environmental factors that exert pressure on our health has not kept pace with genomics and historically has not provided a similar level of resolution. Emerging technologies make it possible to obtain detailed information on drugs, toxicants, pollutants, nutrients, and physical and psychological stressors on an omic scale. These forces can also be assessed at systems and network levels, providing a framework for advances in pharmacology and toxicology. The exposome paradigm can improve the analysis of drug interactions and detection of adverse effects of drugs and toxicants and provide data on biological responses to exposures. The comprehensive model can provide data at the individual level for precision medicine, group level for clinical trials, and population level for public health.},
	language = {eng},
	journal = {Annual Review of Pharmacology and Toxicology},
	author = {Niedzwiecki, Megan M. and Walker, Douglas I. and Vermeulen, Roel and Chadeau-Hyam, Marc and Jones, Dean P. and Miller, Gary W.},
	year = {2019},
	pmid = {30095351},
	keywords = {systems biology, epidemiology, metabolomics, exposome, exposure, pharmacokinetics},
	pages = {107--127}
}

@article{vermeulen_exposome_nodate,
	title = {The exposome and health: where chemistry meets biology.},
	volume = {In press},
	author = {Vermeulen, Roel and Schymanski, Emma L. and Albert-Laszlo, Barabasi and Miller, Gary W.}
}

@article{meng_dimension_2016,
	title = {Dimension reduction techniques for the integrative analysis of multi-omics data},
	volume = {17},
	number = {4},
	journal = {Briefings in bioinformatics},
	author = {Meng, Chen and Zeleznik, Oana A. and Thallinger, Gerhard G. and Kuster, Bernhard and Gholami, Amin M. and Culhane, Aedín C.},
	year = {2016},
	note = {ISBN: 1477-4054
Publisher: Oxford University Press},
	pages = {628--641}
}

@article{mirza_machine_2019,
	title = {Machine {Learning} and {Integrative} {Analysis} of {Biomedical} {Big} {Data}},
	volume = {10},
	issn = {2073-4425},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6410075/},
	doi = {10.3390/genes10020087},
	abstract = {Recent developments in high-throughput technologies have accelerated the accumulation of massive amounts of omics data from multiple sources: genome, epigenome, transcriptome, proteome, metabolome, etc. Traditionally, data from each source (e.g., genome) is analyzed in isolation using statistical and machine learning (ML) methods. Integrative analysis of multi-omics and clinical data is key to new biomedical discoveries and advancements in precision medicine. However, data integration poses new computational challenges as well as exacerbates the ones associated with single-omics studies. Specialized computational approaches are required to effectively and efficiently perform integrative analysis of biomedical data acquired from diverse modalities. In this review, we discuss state-of-the-art ML-based approaches for tackling five specific computational challenges associated with integrative analysis: curse of dimensionality, data heterogeneity, missing data, class imbalance and scalability issues.},
	number = {2},
	urldate = {2020-03-11},
	journal = {Genes},
	author = {Mirza, Bilal and Wang, Wei and Wang, Jie and Choi, Howard and Chung, Neo Christopher and Ping, Peipei},
	month = jan,
	year = {2019},
	pmid = {30696086},
	pmcid = {PMC6410075}
}

@article{de_ridder_pattern_2013,
	title = {Pattern recognition in bioinformatics},
	volume = {14},
	issn = {1467-5463},
	url = {https://academic.oup.com/bib/article/14/5/633/218439},
	doi = {10.1093/bib/bbt020},
	abstract = {Abstract.  Pattern recognition is concerned with the development of systems that learn to solve a given problem using a set of example instances, each represent},
	language = {en},
	number = {5},
	urldate = {2020-03-11},
	journal = {Briefings in Bioinformatics},
	author = {de Ridder, Dick and de Ridder, Jeroen and Reinders, Marcel J. T.},
	month = sep,
	year = {2013},
	note = {Publisher: Oxford Academic},
	pages = {633--647}
}

@article{jain_data_1999,
	title = {Data clustering: a review},
	volume = {31},
	issn = {0360-0300},
	shorttitle = {Data clustering},
	url = {https://doi.org/10.1145/331499.331504},
	doi = {10.1145/331499.331504},
	abstract = {Clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines; this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. However, clustering is a difficult problem combinatorially, and differences in assumptions and contexts in different communities has made the transfer of useful generic concepts and methodologies slow to occur. This paper presents an overview of pattern clustering methods from a statistical pattern recognition perspective, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners. We present a taxonomy of clustering techniques, and identify cross-cutting themes and recent advances. We also describe some important applications of clustering algorithms such as image segmentation, object recognition, and information retrieval.},
	number = {3},
	urldate = {2020-03-11},
	journal = {ACM Computing Surveys},
	author = {Jain, A. K. and Murty, M. N. and Flynn, P. J.},
	month = sep,
	year = {1999},
	keywords = {unsupervised learning, cluster analysis, clustering applications, exploratory data analysis, incremental clustering, similarity indices},
	pages = {264--323}
}

@article{bartenhagen_comparative_2010,
	title = {Comparative study of unsupervised dimension reduction techniques for the visualization of microarray gene expression data},
	volume = {11},
	issn = {1471-2105},
	url = {https://doi.org/10.1186/1471-2105-11-567},
	doi = {10.1186/1471-2105-11-567},
	abstract = {Visualization of DNA microarray data in two or three dimensional spaces is an important exploratory analysis step in order to detect quality issues or to generate new hypotheses. Principal Component Analysis (PCA) is a widely used linear method to define the mapping between the high-dimensional data and its low-dimensional representation. During the last decade, many new nonlinear methods for dimension reduction have been proposed, but it is still unclear how well these methods capture the underlying structure of microarray gene expression data. In this study, we assessed the performance of the PCA approach and of six nonlinear dimension reduction methods, namely Kernel PCA, Locally Linear Embedding, Isomap, Diffusion Maps, Laplacian Eigenmaps and Maximum Variance Unfolding, in terms of visualization of microarray data.},
	number = {1},
	urldate = {2020-03-11},
	journal = {BMC Bioinformatics},
	author = {Bartenhagen, Christoph and Klein, Hans-Ulrich and Ruckert, Christian and Jiang, Xiaoyi and Dugas, Martin},
	month = nov,
	year = {2010},
	pages = {567}
}

@article{smola_advances_1999,
	title = {Advances in kernel methods: support vector learning},
	author = {Smola, Alex},
	year = {1999},
	note = {Publisher: Citeseer}
}

@article{scholkopf_nonlinear_1998,
	title = {Nonlinear component analysis as a kernel eigenvalue problem},
	volume = {10},
	number = {5},
	journal = {Neural computation},
	author = {Schölkopf, Bernhard and Smola, Alexander and Müller, Klaus-Robert},
	year = {1998},
	note = {ISBN: 0899-7667
Publisher: MIT Press},
	pages = {1299--1319}
}

@incollection{cox_multidimensional_2008,
	title = {Multidimensional scaling},
	booktitle = {Handbook of data visualization},
	publisher = {Springer},
	author = {Cox, Michael AA and Cox, Trevor F.},
	year = {2008},
	pages = {315--347}
}

@article{tenenbaum_global_2000,
	title = {A global geometric framework for nonlinear dimensionality reduction},
	volume = {290},
	number = {5500},
	journal = {science},
	author = {Tenenbaum, Joshua B. and De Silva, Vin and Langford, John C.},
	year = {2000},
	note = {ISBN: 0036-8075
Publisher: American Association for the Advancement of Science},
	pages = {2319--2323}
}

@article{tibshirani_regression_1996,
	title = {Regression {Shrinkage} and {Selection} via the {Lasso}},
	volume = {58},
	issn = {0035-9246},
	url = {https://www.jstor.org/stable/2346178},
	abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
	number = {1},
	urldate = {2020-03-11},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Tibshirani, Robert},
	year = {1996},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	pages = {267--288}
}

@article{hoerl_ridge_1970,
	title = {Ridge regression: {Biased} estimation for nonorthogonal problems},
	volume = {12},
	number = {1},
	journal = {Technometrics},
	author = {Hoerl, Arthur E. and Kennard, Robert W.},
	year = {1970},
	note = {ISBN: 0040-1706
Publisher: Taylor \& Francis Group},
	pages = {55--67}
}

@article{zou_regularization_2005,
	title = {Regularization and variable selection via the elastic net},
	volume = {67},
	number = {2},
	journal = {Journal of the royal statistical society: series B (statistical methodology)},
	author = {Zou, Hui and Hastie, Trevor},
	year = {2005},
	note = {ISBN: 1467-9868
Publisher: Wiley Online Library},
	pages = {301--320}
}

@inproceedings{rosipal_overview_2006,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Overview and {Recent} {Advances} in {Partial} {Least} {Squares}},
	isbn = {978-3-540-34138-3},
	doi = {10.1007/11752790_2},
	abstract = {Partial Least Squares (PLS) is a wide class of methods for modeling relations between sets of observed variables by means of latent variables. It comprises of regression and classification tasks as well as dimension reduction techniques and modeling tools. The underlying assumption of all PLS methods is that the observed data is generated by a system or process which is driven by a small number of latent (not directly observed or measured) variables. Projections of the observed data to its latent structure by means of PLS was developed by Herman Wold and coworkers [48,49,52].},
	language = {en},
	booktitle = {Subspace, {Latent} {Structure} and {Feature} {Selection}},
	publisher = {Springer},
	author = {Rosipal, Roman and Krämer, Nicole},
	editor = {Saunders, Craig and Grobelnik, Marko and Gunn, Steve and Shawe-Taylor, John},
	year = {2006},
	keywords = {Canonical Correlation Analysis, Mean Square Error, Partial Little Square, Partial Little Square Regression, Principal Component Regression},
	pages = {34--51}
}

@inproceedings{rosipal_overview_2005,
	title = {Overview and recent advances in partial least squares},
	booktitle = {International {Statistical} and {Optimization} {Perspectives} {Workshop}" {Subspace}, {Latent} {Structure} and {Feature} {Selection}"},
	publisher = {Springer},
	author = {Rosipal, Roman and Krämer, Nicole},
	year = {2005},
	pages = {34--51}
}

@article{le_cao_sparse_2011,
	title = {Sparse {PLS} discriminant analysis: biologically relevant feature selection and graphical displays for multiclass problems},
	volume = {12},
	issn = {1471-2105},
	shorttitle = {Sparse {PLS} discriminant analysis},
	url = {https://doi.org/10.1186/1471-2105-12-253},
	doi = {10.1186/1471-2105-12-253},
	abstract = {Variable selection on high throughput biological data, such as gene expression or single nucleotide polymorphisms (SNPs), becomes inevitable to select relevant information and, therefore, to better characterize diseases or assess genetic structure. There are different ways to perform variable selection in large data sets. Statistical tests are commonly used to identify differentially expressed features for explanatory purposes, whereas Machine Learning wrapper approaches can be used for predictive purposes. In the case of multiple highly correlated variables, another option is to use multivariate exploratory approaches to give more insight into cell biology, biological pathways or complex traits.},
	number = {1},
	urldate = {2020-03-11},
	journal = {BMC Bioinformatics},
	author = {Lê Cao, Kim-Anh and Boitard, Simon and Besse, Philippe},
	month = jun,
	year = {2011},
	pages = {253}
}

@article{nguyen_tumor_2002,
	title = {Tumor classification by partial least squares using microarray gene expression data},
	volume = {18},
	issn = {1367-4803},
	url = {https://academic.oup.com/bioinformatics/article/18/1/39/243615},
	doi = {10.1093/bioinformatics/18.1.39},
	abstract = {Abstract.  Motivation: One important application of gene expression
  microarray data is classification of samples into categories, such
  as the type of tumor.},
	language = {en},
	number = {1},
	urldate = {2020-03-11},
	journal = {Bioinformatics},
	author = {Nguyen, Danh V. and Rocke, David M.},
	month = jan,
	year = {2002},
	note = {Publisher: Oxford Academic},
	pages = {39--50}
}

@article{nguyen_ten_2019,
	title = {Ten quick tips for effective dimensionality reduction},
	volume = {15},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006907},
	doi = {10.1371/journal.pcbi.1006907},
	language = {en},
	number = {6},
	urldate = {2020-03-14},
	journal = {PLOS Computational Biology},
	author = {Nguyen, Lan Huong and Holmes, Susan},
	month = jun,
	year = {2019},
	note = {Publisher: Public Library of Science},
	keywords = {Aspect ratio, Phenols, Linear discriminant analysis, Kernel methods, Data visualization, Eigenvalues, Principal component analysis, Wine},
	pages = {e1006907}
}

@article{li_understanding_2019,
	title = {Understanding mixed environmental exposures using metabolomics via a hierarchical community network model in a cohort of {California} women in 1960’s},
	issn = {0890-6238},
	url = {http://www.sciencedirect.com/science/article/pii/S0890623818306038},
	doi = {10.1016/j.reprotox.2019.06.013},
	abstract = {Even though the majority of population studies in environmental health focus on a single factor, environmental exposure in the real world is a mixture of many chemicals. The concept of “exposome” leads to an intellectual framework of measuring many exposures in humans, and the emerging metabolomics technology offers a means to read out both the biological activity and environmental impact in the same dataset. How to integrate exposome and metabolome in data analysis is still challenging. Here, we employ a hierarchical community network to investigate the global associations between the metabolome and mixed exposures including DDTs, PFASs and PCBs, in a women cohort with sera collected in California in the 1960s. Strikingly, this analysis revealed that the metabolite communities associated with the exposures were non-specific and shared among exposures. This suggests that a small number of metabolic phenotypes may account for the response to a large class of environmental chemicals.},
	language = {en},
	urldate = {2020-03-14},
	journal = {Reproductive Toxicology},
	author = {Li, Shuzhao and Cirillo, Piera and Hu, Xin and Tran, ViLinh and Krigbaum, Nickilou and Yu, Shaojun and Jones, Dean P. and Cohn, Barbara},
	month = jul,
	year = {2019},
	keywords = {Metabolomics, Exposome, DDT, Metabolic phenotype, Breast cancer, Gene environment interaction, Hierarchical community network, Mixed exposures, Multi-omics integration, MWAS, PCB, PFAS, Variance analysis}
}

@article{scholkopf_nonlinear_1998-1,
	title = {Nonlinear component analysis as a kernel eigenvalue problem},
	volume = {10},
	number = {5},
	journal = {Neural computation},
	author = {Schölkopf, Bernhard and Smola, Alexander and Müller, Klaus-Robert},
	year = {1998},
	note = {ISBN: 0899-7667
Publisher: MIT Press},
	pages = {1299--1319}
}

@article{vermeulen_exposome_2020,
	title = {The exposome and health: {Where} chemistry meets biology},
	volume = {367},
	copyright = {Copyright © 2020, American Association for the Advancement of Science. http://www.sciencemag.org/about/science-licenses-journal-article-reuseThis is an article distributed under the terms of the Science Journals Default License.},
	issn = {0036-8075, 1095-9203},
	shorttitle = {The exposome and health},
	url = {https://science.sciencemag.org/content/367/6476/392},
	doi = {10.1126/science.aay3164},
	abstract = {{\textless}p{\textgreater}Despite extensive evidence showing that exposure to specific chemicals can lead to disease, current research approaches and regulatory policies fail to address the chemical complexity of our world. To safeguard current and future generations from the increasing number of chemicals polluting our environment, a systematic and agnostic approach is needed. The “exposome” concept strives to capture the diversity and range of exposures to synthetic chemicals, dietary constituents, psychosocial stressors, and physical factors, as well as their corresponding biological responses. Technological advances such as high-resolution mass spectrometry and network science have allowed us to take the first steps toward a comprehensive assessment of the exposome. Given the increased recognition of the dominant role that nongenetic factors play in disease, an effort to characterize the exposome at a scale comparable to that of the human genome is warranted.{\textless}/p{\textgreater}},
	language = {en},
	number = {6476},
	urldate = {2020-03-14},
	journal = {Science},
	author = {Vermeulen, Roel and Schymanski, Emma L. and Barabási, Albert-László and Miller, Gary W.},
	month = jan,
	year = {2020},
	pmid = {31974245},
	note = {Publisher: American Association for the Advancement of Science
Section: Review},
	pages = {392--396}
}

@article{kingma_auto-encoding_2014,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2020-03-30},
	journal = {arXiv:1312.6114 [cs, stat]},
	author = {Kingma, Diederik P. and Welling, Max},
	month = may,
	year = {2014},
	note = {arXiv: 1312.6114},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}